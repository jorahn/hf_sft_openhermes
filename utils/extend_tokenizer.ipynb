{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrahn/anaconda3/envs/hf/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "output = \"../out/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(output)\n",
    "model = AutoModelForCausalLM.from_pretrained(output, attn_implementation=\"flash_attention_2\", torch_dtype=torch.bfloat16, device_map='cuda')\n",
    "model.eval()\n",
    "tokens = tokenizer.encode(\"During photosynthesis in green plants\", return_tensors=\"pt\")\n",
    "tokens = tokens.to('cuda')\n",
    "output = model.generate(tokens, max_new_tokens=64, repetition_penalty=1.3)\n",
    "samples = tokenizer.batch_decode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|endoftext|>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<|endoftext|>'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a friendly chatbot who always responds in the style of a pirate<|endoftext|>How many helicopters can a human eat in one sitting?<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    " ]\n",
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "print(tokenizer.decode(tokenized_chat[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a friendly chatbot who always responds in the style of a pirate<|im_end|>\n",
      "<|im_start|>user\n",
      "How many helicopters can a human eat in one sitting?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\", add_special_tokens=True)\n",
    "print(tokenizer.decode(tokenized_chat[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Sonnet:\n",
    "\n",
    "Based on your situation, it sounds like you've trained a language model from scratch and now want to adapt it for instruction following using a chat format. This is a common progression, and using chat templates can be very helpful for this task. Let's break down the key points and steps you should consider:\n",
    "\n",
    "1. Adding special tokens:\n",
    "   Your idea to add special tokens like <|im_start|> is a good one. These tokens can help delineate different parts of the conversation and roles. Here's how you might add new special tokens:\n",
    "\n",
    "   ```python\n",
    "   new_tokens = [\"<|im_start|>\", \"<|im_end|>\", \"<|user|>\", \"<|assistant|>\"]\n",
    "   num_added_tokens = tokenizer.add_tokens(new_tokens, special_tokens=True)\n",
    "   \n",
    "   # Resize the token embeddings as we've added new tokens\n",
    "   model.resize_token_embeddings(len(tokenizer))\n",
    "   ```\n",
    "\n",
    "2. Creating a chat template:\n",
    "   After adding the special tokens, you'll want to create a chat template. Here's an example of what it might look like:\n",
    "\n",
    "   ```python\n",
    "   chat_template = \"\"\"<|im_start|>{role}\n",
    "   {content}<|im_end|>\n",
    "   \"\"\"\n",
    "   \n",
    "   tokenizer.chat_template = chat_template\n",
    "   ```\n",
    "\n",
    "3. Using the chat template:\n",
    "   Once you've set up the chat template, you can use it like this:\n",
    "\n",
    "   ```python\n",
    "   messages = [\n",
    "       {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "       {\"role\": \"assistant\", \"content\": \"I'm doing well, thank you! How can I assist you today?\"},\n",
    "       {\"role\": \"user\", \"content\": \"Can you explain what chat templates are?\"}\n",
    "   ]\n",
    "   \n",
    "   model_input = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "   ```\n",
    "\n",
    "4. Training considerations:\n",
    "   When fine-tuning on instruction data, you'll want to format your training data using the chat template. You can do this as a preprocessing step:\n",
    "\n",
    "   ```python\n",
    "   def format_instruction(example):\n",
    "       return tokenizer.apply_chat_template(example['messages'], tokenize=False, add_generation_prompt=False)\n",
    "   \n",
    "   dataset = dataset.map(format_instruction)\n",
    "   ```\n",
    "\n",
    "   Then, when you tokenize for training, make sure to set `add_special_tokens=False`:\n",
    "\n",
    "   ```python\n",
    "   tokenized_dataset = dataset.map(lambda x: tokenizer(x['formatted_instruction'], add_special_tokens=False))\n",
    "   ```\n",
    "\n",
    "5. Saving the changes:\n",
    "   After making these changes, don't forget to save your tokenizer:\n",
    "\n",
    "   ```python\n",
    "   tokenizer.save_pretrained(\"path/to/save\")\n",
    "   ```\n",
    "\n",
    "Remember, when you're using `apply_chat_template` with `tokenize=False` and then tokenizing separately, you should set `add_special_tokens=False` in the tokenization step to avoid adding extra special tokens that might interfere with your chat template.\n",
    "\n",
    "These steps should help you transition your model to an instruction-following chat format. The chat templating system in Transformers is flexible, so you can adjust the template to fit your specific needs.\n",
    "\n",
    "Would you like me to elaborate on any part of this process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 1024)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(['<|im_start|>', '<|im_end|>'], special_tokens=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tk = \"../out_tk/\"\n",
    "tokenizer.save_pretrained(output_tk)\n",
    "model.save_pretrained(output_tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'messages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# check if the new tokens are added\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m tokenized_chat \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(\u001b[43mmessages\u001b[49m, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(tokenized_chat[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenized_chat)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'messages' is not defined"
     ]
    }
   ],
   "source": [
    "# check if the new tokens are added\n",
    "\n",
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\", add_special_tokens=True)\n",
    "print(tokenizer.decode(tokenized_chat[0]))\n",
    "print(tokenized_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "output = \"../out_tk/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(output)\n",
    "model = AutoModelForCausalLM.from_pretrained(output, attn_implementation=\"flash_attention_2\", torch_dtype=torch.bfloat16, device_map='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['custom_instruction', 'topic', 'model_name', 'model', 'skip_prompt_formatting', 'category', 'conversations', 'views', 'language', 'id', 'title', 'idx', 'hash', 'avatarUrl', 'system_prompt', 'source'],\n",
       "    num_rows: 1001551\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"teknium/OpenHermes-2.5\")\n",
    "ds[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'from': 'human',\n",
       "   'value': 'Every day, a tree drops 7 leaves. How many leaves would it drop in a month of February in a non-leap year? Include your logic.',\n",
       "   'weight': None},\n",
       "  {'from': 'gpt',\n",
       "   'value': \"Here's the logic behind this:\\n\\n1. We know that February has 28 days in a non-leap year.\\n2. If the tree drops 7 leaves every day, then over the course of February, it would drop:\\n   Leaves dropped in February = Leaves per day * Days in February\\n   = 7 leaves * 28 days\\n   = 196 leaves\\n\\nSo, the tree would drop 196 leaves in February in a non-leap year.\",\n",
       "   'weight': None}],\n",
       " [{'from': 'human',\n",
       "   'value': 'In analytical chemistry, what is the principle behind the use of an internal standard in quantitative analysis?\\nA. It compensates for variations in sample preparation and instrumental response.\\nB. It enhances the sensitivity of the analytical method.\\nC. It reduces the detection limit of the analytical method.\\nD. It increases the resolution between analyte peaks in chromatography.\\nE. None of the above.',\n",
       "   'weight': None},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'A. It compensates for variations in sample preparation and instrumental response.',\n",
       "   'weight': None}],\n",
       " [{'from': 'human',\n",
       "   'value': 'A rectangular garden has a length of 25 feet and a width of 15 feet. If you want to build a fence around the entire garden, how many feet of fencing will you need?',\n",
       "   'weight': None},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'To determine the amount of fencing needed, we first need to calculate the perimeter of the rectangle. The formula for the perimeter of a rectangle is P = 2(L+W), where L is the length and W is the width.\\n\\nPerimeter = 2(L+W)\\n                 = 2(25 feet + 15 feet)\\n                 = 2(40 feet)\\n                 = 80 feet\\n\\nSo, you will need 80 feet of fencing to enclose the entire garden.',\n",
       "   'weight': None}],\n",
       " [{'from': 'human',\n",
       "   'value': 'What was the purpose of the Colosseum in Rome?',\n",
       "   'weight': None},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The Colosseum in Rome was used for various public spectacles and events. These included gladiatorial contests, mock sea battles, animal hunts, executions, re-enactments of famous battles, and dramas based on Classical mythology. It was a symbol of the grandeur and power of the Roman Empire and was a place for the emperor to connect with the people by providing them with entertainment and free food.',\n",
       "   'weight': None}],\n",
       " [{'from': 'human',\n",
       "   'value': \"A man claims he can predict the outcome of a dice roll with 90% accuracy. If he predicts correctly on his first attempt, what is the probability that he actually has the ability to predict dice rolls with 90% accuracy? Assume there's a 1% chance that someone can have this ability.\",\n",
       "   'weight': None},\n",
       "  {'from': 'gpt',\n",
       "   'value': \"Let's use Bayes' theorem again to solve this problem:\\n\\nLet A represent the event that the man actually has the ability to predict dice rolls with 90% accuracy, and C represent the event of predicting correctly on the first attempt.\\n\\nWe want to find P(A|C), the probability that the man actually has the ability given that he predicted correctly on his first attempt.\\n\\nBayes' theorem states that P(A|C) = P(C|A) * P(A) / P(C)\\n\\nFirst, let's find P(C|A): the probability of predicting correctly on the first attempt if the man actually has the ability. Since he claims 90% accuracy, this probability is 0.9.\\n\\nNext, let's find P(A): the probability that someone actually has the ability to predict dice rolls with 90% accuracy. We are told this is 1%, so P(A) = 0.01.\\n\\nNow we need to find P(C): the overall probability of predicting correctly on the first attempt. This can be calculated as the sum of probabilities for each case: P(C) = P(C|A) * P(A) + P(C|¬A) * P(¬A), where ¬A represents not having the ability and P(¬A) = 1 - P(A) = 0.99.\\n\\nTo find P(C|¬A), the probability of predicting correctly on the first attempt without the ability, we use the fact that there's a 1/6 chance of guessing correctly by random chance: P(C|¬A) = 1/6.\\n\\nSo, P(C) = (0.9)*(0.01) + (1/6)*(0.99) = 0.009 + 0.165 = 0.174.\\n\\nFinally, we can calculate P(A|C) using Bayes' theorem:\\n\\nP(A|C) = P(C|A) * P(A) / P(C) = (0.9)*(0.01) / (0.174) ≈ 0.0517.\\n\\nTherefore, the probability that the man actually has the ability to predict dice rolls with 90% accuracy is approximately 5.17%.\",\n",
       "   'weight': None}]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show one example from conversions feature in the dataset\n",
    "ds[\"train\"][:5][\"conversations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1001551/1001551 [02:18<00:00, 7226.14 examples/s]\n",
      "Map: 100%|██████████| 1001551/1001551 [12:17<00:00, 1358.52 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def sharegpt_to_chatml(example):\n",
    "    chatml_conversations = []\n",
    "    for conv in example[\"conversations\"]:\n",
    "        if conv[\"from\"] == \"human\":\n",
    "            role = \"user\"\n",
    "        elif conv[\"from\"] == \"system\":\n",
    "            role = \"system\"\n",
    "        elif conv[\"from\"] == \"gpt\":\n",
    "            role = \"assistant\"\n",
    "        else:\n",
    "            role = \"user\"\n",
    "        chatml_format = {\"role\": role, \"content\": conv[\"value\"]}\n",
    "        chatml_conversations.append(chatml_format)\n",
    "    return {\"conversations\": chatml_conversations}\n",
    "\n",
    "def format_conversation(example):\n",
    "    formatted = tokenizer.apply_chat_template(example['conversations'], tokenize=False, add_generation_prompt=False)\n",
    "    return {\"formatted_conversation\": formatted}\n",
    "\n",
    "def tokenize_conversation(example):\n",
    "    return tokenizer(example['formatted_conversation'], truncation=True, max_length=1024, add_special_tokens=False)\n",
    "\n",
    "ds = ds.map(sharegpt_to_chatml)\n",
    "ds_format = ds.map(format_conversation)\n",
    "ds_tokenized = ds_format.map(tokenize_conversation, remove_columns=['conversations', 'formatted_conversation'])\n",
    "ds_tokenized.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (5/5 shards): 100%|██████████| 1001551/1001551 [00:02<00:00, 403372.09 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds_tokenized.save_to_disk(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tokenized = load_dataset(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_tokenized[\"train\"][9][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='375582' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [     3/375582 00:05 < 615:32:37, 0.17 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m\n\u001b[1;32m      3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      4\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     save_total_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     12\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     13\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     14\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mDataCollatorForLanguageModeling(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     15\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mds_tokenized[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.11/site-packages/transformers/trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.11/site-packages/transformers/trainer.py:2273\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2268\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2271\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2272\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2273\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2274\u001b[0m ):\n\u001b[1;32m   2275\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2276\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    train_dataset=ds_tokenized[\"train\"],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
